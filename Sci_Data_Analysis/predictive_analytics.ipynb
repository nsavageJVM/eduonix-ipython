{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python library imports: numpy, random, sklearn, pandas, etc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import linear_model, cross_validation, metrics, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "from patsy import dmatrices\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to read HDFS file into dataframe using PyDoop\n",
    "import pydoop.hdfs as hdfs\n",
    "def read_csv_from_hdfs( path ):\n",
    "  pieces = []\n",
    "  fhandle = hdfs.open(path)\n",
    "  print \"validating file : %s\" % fhandle\n",
    "  cols = ['key', 'value'];\n",
    "  pieces.append(pd.read_csv(fhandle, names=cols, dtype=None, delimiter=\"\\t\"))\n",
    "  fhandle.close()\n",
    "  return pd.concat(pieces, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_data_as_frame(in_data ):\n",
    "    # dataset LONGITUDE  LATITUDE  T_DAILY_MEAN  SUR_TEMP_DAILY_AVG  SOIL_MOISTURE_10_DAILY\n",
    "    data_list = []\n",
    "    for index in data_val2:\n",
    "        dict1 = {}\n",
    "        x= float(index[3])  \n",
    "        if x < -30:\n",
    "            continue\n",
    "        x= float(index[2])  \n",
    "        if x < -30:\n",
    "            continue\n",
    "        dict1.update(lat=float(index[1]),lon=float(index[0]), day=float(index[2]), surface=float(index[3]), moisture=float(index[4])) \n",
    "        data_list.append(dict1)\n",
    "    data_as_frame = pd.DataFrame(data_list, columns=['lat', 'lon', 'day', 'surface', 'moisture'])\n",
    "    return data_as_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_geo_data(in_data ):\n",
    "    geo_list = []\n",
    "    for index in data_val2:\n",
    "        dict1 = {}\n",
    "        dict1.update(lat=index[1],lon=index[0]) \n",
    "        geo_list.append(dict1)\n",
    "    geo_key = pd.DataFrame(geo_list, columns=['lat', 'lon'])\n",
    "    return geo_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_temp_data(in_data ):\n",
    "    temp_list = []\n",
    "    for index in data_val2:\n",
    "        dict1 = {}\n",
    "        dict1.update(day=index[2],surface=index[3]) \n",
    "        temp_list.append(dict1)\n",
    "    temp_values = pd.DataFrame(temp_list, columns=['day', 'surface'])\n",
    "    return temp_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_soil_mositure(in_data ):\n",
    "    moisture_list = []\n",
    "    for index in data_val2:\n",
    "        dict1 = {}\n",
    "        dict1.update(moisture=index[4] ) \n",
    "        moisture_list.append(dict1)\n",
    "    moisture_values = pd.DataFrame(moisture_list, columns=['moisture'])\n",
    "    return moisture_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating file : <pydoop.hdfs.file.hdfs_file object at 0x5829d90>\n",
      "Index([u'Intercept', u'lat', u'lon', u'day', u'surface'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>[-0.0280144477647, -0.053052304366, 0.01672769...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lat</td>\n",
       "      <td>[-0.689967315305, -0.153928247361, 0.211091392...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lon</td>\n",
       "      <td>[0.388184946552, 0.0697258788407, -0.153419980...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>day</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>surface</td>\n",
       "      <td>[0.165362569539, -0.0523609530014, -0.24406524...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0                                                  1\n",
       "0  Intercept  [-0.0280144477647, -0.053052304366, 0.01672769...\n",
       "1        lat  [-0.689967315305, -0.153928247361, 0.211091392...\n",
       "2        lon  [0.388184946552, 0.0697258788407, -0.153419980...\n",
       "3        day                [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
       "4    surface  [0.165362569539, -0.0523609530014, -0.24406524..."
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = read_csv_from_hdfs('/user/cloudera/sci_data_1_out.txt/part-r-00000')\n",
    "data_val =  result.iloc[:,[1]]\n",
    "# data_val2 will be a series so we convert it to useful dataframes \n",
    "# dataset LONGITUDE  LATITUDE  T_DAILY_MEAN  SUR_TEMP_DAILY_AVG  SOIL_MOISTURE_10_DAILY\n",
    "data_val2 = data_val.value.str.split('|')\n",
    "data_as_frame =extract_data_as_frame(data_val2 ) \n",
    "\n",
    "cols = ['lat', 'lon', 'day', 'surface', 'moisture']\n",
    "dataNorth_y = data_as_frame[data_as_frame['lat'] > 23.8 ]\n",
    "dataNorth_x = data_as_frame[cols]\n",
    "y, X = dmatrices('moisture ~ lat + lon + day + surface', dataNorth_y, return_type=\"dataframe\")\n",
    "print X.columns\n",
    "y = np.ravel(y)\n",
    "model = LogisticRegression()\n",
    "model = model.fit(X, y)\n",
    "y.mean()\n",
    "pd.DataFrame(zip(X.columns, np.transpose(model.coef_)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}